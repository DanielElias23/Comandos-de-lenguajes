











---------------------------------------------------------------------------------------------------------------------------
                                                 MACHING LEARNING SUPERVISADO
---------------------------------------------------------------------------------------------------------------------------

                                                     COMANDOS GENERALES




--------------------------------------------------------------------------------------------------------------------------
        
                                                      PREPROCESAMIENTO

Para ocupar cada funcion como son muy largos los nombres se pasan a variables y luego para hacer efectivos los cambios
se pasan a array con un transformador

Ejemplo: OHE = OneHotEncoder()
         OHE.fit_transform(datos)   #OHE se queda con los datos que le pasaron, estos datos estaran por defecto

OHE.transform(columna)  ##transform es para aplicar la transformacion a la columna (codificacion en este caso)

OHE.inverse_transform(resulado_anterior)   ##deshace el cambio anterior, vuelve a la normalidad

OHE.classes_   ##muestra la cantidad de datos uncicos dentro del fit

####         


from sklearn.preproccessing import OneHotEncoder

OneHotEncoder()  OneHotEncoder()                               "Una columna de catergoria con string, la divide en varias columnas
                                                                dependiendo de la cantidad de categorias, ejemplo 3 categorias
                                                                las divide en 3 columna, siendo el nombre de la columna la
                                                                categoria anterior, si la categoria de esa FILA es una la
                                                                marcara con 1 en esa columna y 0 cuando no lo es, en todas las
                                                                otras categorias marcara 0, ejemplo: [0, 1, 0] o [1, 0, 0]"

from sklearn.preproccessing import LabelsEncoder

LabelsEncoder()  LabelsEncoder()                               "Una columna con categorias con string, simplemente las
                                                                cambia de string a valor numerico, asignandole un numero
                                                                entero a cada categoria, ejemplo: "Gris"  a valor 1,
                                                                "Verde" a valor 2, etc y crea una nueva columna"

from sklearn.preproccessing import MinMaxScaler

MinMaxScaler()  MinMaxScaler()                                 "Lo que hace es escalar los valores numericos entre 0 y 1,
                                                                siendo por ejemplo una columna con valores, el mayor
                                                                valor de esa columna, sera 1 y el menor sera 0, todos los otros
                                                                estaran en la misma relacion unos a otros, pero seran valores
                                                                entre 0 y 1, hay una formula para esto"


from sklearn.preproccessing import StandardScaler

StandardScaler()  StandardScaler()                             "Traspasa todos los datos a la otra escala, los datos son
                                                                modificados para que sean valores oscilando cerca de 0,
                                                                pero la relacion entre todos se mantiene, al graficarlos
                                                                se conserva la misma forma"

from sklearn.preproccessing import PolynomialFeatures

PolynomialFeatures()  PolynomialFeatures(degree=2, include_bias=True)
                                       
                                                               "Se dice que les crea mas categorias a los datos, pero lo que
                                                                es crea un nuevo array con valores que se le aplican ciertas
                                                                operaciones matematicas (Se le llama polynomica) es como si
                                                                expandiera el dato en mas datos"

                      pf = PolynomialFeatures(degree=2, include_bias=True)
                      pf.fit.transform([2,3])
                                                               "Lo que hace es [2,3], como le dijimos bias si agrega un valor
                                                                1, entonces lo que hace es crea una nueva tupla
                                                                con en este caso 6 valores porque es grado dos, hace las
                                                                siguientes operaciones: [1, 2^1, 3^1, 2^2, 3^2, 2*3] y asi
                                                                sucesivamente en caso de agregar mas grados"

                      X_poly = pf.fit.transform(data[["x"]])   "Para ocupar caracteristicas polinomiales a una columna
                                                                tenemos que ponerla en doble corchete"

                      lr = LinearRegresion.fit(X_poly, Y_data) 

                      Y_pred = lr.predict(X_poly)                                                               


from sklearn.preproccessing import LabelBinarizer

LabelBinarizer()  LabelBinarizer()                             "Genera una matriz en binario, desde una columna en el fit
                                                                lo que hara [2,3] dentro del fit entendera como que solo
                                                                quieres mostrar la columna 2 y 3, el resultado seran dos
                                                                columna, matriz de nx2, en el transform va otra tupla, con
                                                                [2,4] que cada valor indica donde van los 1, todo esta con 0,
                                                                excepto las columnas [2,4], la 4 en este caso no se muestra,
                                                                por que en el fit no se le especifico.

                                                                [[1,0]  Ira poniendo los 1 fila por fila, fila 1, columna 2,
                                                                [0,0]]  colocara un 1, (la columna 2 es la que se muestra
                                                                        de las primeras, la cantidad de filas que tenga
                                                                        sera debido a la cantidad de datos en transform)

from sklearn.preproccessing import OrdinalEncoder
                                                                        
OrdinalEncoder() OrdinalEncoder()                              "Al entregarle una tupla con subtuplas [[,],[,]] al fit
                                                                lo que hara sera darle un numero a cada dato, puede ser string
                                                                0 al primero que encuentre en caso de string, 0 al numero mas
                                                                bajo que encuentre, diferenciara automaticamente si son numeros
                                                                o string, creado dos codificaciones simultaneras para cada tipo
                                                                de dato, lo que hara sera entregar la misma tupla pero solo con
                                                                numeros que categorizan y se refieren a una categoria. Crea
                                                                una matriz solo con los datos que le especifiquemos en el
                                                                transform([[,],[,]]), pero la categorizacion se creo en el 
                                                                fit.



---------------------------------------------------------------------------------------------------------------------------

                                                  REGRESIONES Y CLASIFICADORES

from sklearn.linear_model import LinearRegression    


LinearRegression()  clf = LinearRegression()                   "Crea un modelo para regresion lineal (le pasa las formulas)"

fit()  clf.fit(x,y)                                            "Entrena el modelo creado (hace una regresion lineal)"

coef_  clf.coef_                                               "Nos entrega el valor "b0" para esos datos"

intercept_  clf.intercept_                                     "Nos entrega el valor "b1" para esos datos"

predict()  clf.predict([[x]])  clf.predict(x) (para una columna entera)

                                                               "Segun nuestro modelo nos predice un valor para ese x,
                                                                todo esto clf.predict(x) = y_predecido"


from sklearn.linear_model import Ridge

Ridge()  Ridge(columna, max_iter=100, alpha=0.001)             "Es un otro tipo de regresion que tiene penalizacion por
                                                                por un parametro beta y otro lamnda, lo que voy a buscar
                                                                en esta regresion es que los parametros sean peque√±os, se
                                                                puede minimizar, alpha defina el valor del parametro"


from sklearn.linear_model import Lasso

Lasso()  Lasso(columna, max_iter=100)                          "Es otra regresion, pero esta no se puede minimizar con
                                                                derivada, hay que probar por intento, el beneficio es
                                                                que se puede anular un parametro en la minimizacion"


from sklearn.linear_model import ElasticNetCV

ElasticNetCV()  ElasticNet(alphas, ll_ratio, max_iter=1000).fit(X_train, y_train)

                                                               "Es una mezcla de las regresiones Ridge y Lasso, ademas con
                                                                GridSearch, alphas y ll_ratio son los coeficientes,
                                                                pueden ser listas de datos"

from sklearn.linear_model import LogisticRegression

LogisticRegression()  LogisticRegression(penalty="l2", c=10, solver="liblinear")

                                                               "Hace la regresion logistica, con penality es para penalizar
                                                                con el agregado de lasso y el valor de la constante de
                                                                regulacion 10, "c" mas alta menos penalizacion"                                                                

from sklearn.neighbors import KNeighborsClassifier

KNeighborsClassifier()  KNeighborsClassifier(n_neighbors=3)    "Para hacer una classificacion KNN donde n_neghbors es k,
                                                                tambien posteriormente ocupa fit y prefict"

from sklean.svm import LinearSVC

LinearSVC() LinearSVC(penalty="12", C=10)                      "Es para ocupar la clasificacion support vector machine,
                                                                donde le especificamos la regulacion de los coeficientes,
                                                                tenemos que entregarle un fit(x_train, y_train)

from sklearn.svm import SVC

SVC()  SCV(kernel="rbf", gamma=1, C=10)                        "Es para ocupar la clasificacion support vector machine, pero
                                                                en este caso la version no lineal, ocupando nucleos, gamma
                                                                y C son coeficientes de regulacion, valores bajos para ambos
                                                                es mas regulacion y valores, mas altos menos regulacion"

from sklearn.kernel_approximation import Nystroem

Nystroem()  Nystroem(kernel="rbf", gamma=1, C=10)

from sklearn.kernel_approximation import RBFsampler

RBFsampler()  RBFsampler(gamma=1, n_components=100)


from sklearn.tree import DecisionTreeClassifier

DecisionTreeClassifier()  DecisionTreeClassifier(criterion="Gini", max_features=10, max_depth=5, random_state=0)

                                                              "Para ocupar un arbol de decision con criterio y error de Gini
                                                               con profundidad 5, se le entregan los datos de entrenamiento
                                                               al fit(x_train, y_train)"

tree_  DecisionTreeClassifier.tree_                           "Es para ver los datos del arbol que estamos estudiando"

node_count DecisionTreeClassifier.tree_.node_count            "Muestra la cantidad de nodos del arbol"

max_depth  DecisionTreeClassifier.tree_.max_depth             "Dice la profundidad del arbol"



from sklearn.tree import DecisionTreeRegressor

DecisionTreeRegressor()  DecisionTreeRegressor()              "Es para hacer regresiones con arbol de deciciones, tiene que
                                                               tener un fit"

from sklearn.tree import export_graphvis
from io import StringIO
import pydotplus


dot_data=StringIO()

export_graphvis()  export_graphvis(DecisionTreeClassifier(), out_file=dot_data, filled=True)

                                                               "Para graficar los arboles de decision"

from sklearn.ensemble import BaggingClassifier

BaggingClassifier()  BaggingClassifier(n_estimators=50)        "Es para hacer un empaquetado de arboles de decicion, donde
                                                                el numero de estimadores, es el numero de arboles que tendremos,
                                                                necesita fit(x_train, y_train)"

from sklearn.ensemble import BaggingRegressor

from sklearn.ensemble import RandomForestClassifier

RandomForestClassifier()  RandomForestClassifier(n_estimators=50)

from sklearn.ensemble import ExtraTreeClassifier

ExtraTreeClassifier()  ExtraTreeClassifier(n_estimators=50)

from sklearn.ensemble import RandomForestRegressor

from sklearn.ensemble import GradientBoostingClassifier

GradientBoostingClassifier()  GradientBoostingClassifier(learning_rate=0.1, max_features=1, subsample=0.5, n_estimators=200)

                                                               "Al gradiente le aplican fit(x_train, y_train) luego predict"

from sklearn.ensemble import AdaBoostClassifier

AdaBoostClassifier()  AdaBoostClassifier(base_estimators=DecisionTreeClassifier(), learning_rate=0.1, n_estimators=200)

                                                               "A adaboost se le aplicar el fit y luego predict"

from sklearn.ensemble import VotingClassifier

VotingClassifier() VotingClassifier(lista_de_estimadores)      "Esta funcion es para apilar (stacking) clasificadores,
                                                                como bosques aleatorios, regresion logistica, etc, luego
                                                                se ocupar fit(x_train, y_train) y predict"

from sklearn.ensemble import StackingClassifier

StackingClassifier()  StackingClassifier(lista_de_estimadores, final_estimator=LogisticRegression())                    

                                                               "Es similar al anterior, permite ocupar diferentes
                                                                clasificadores, votar y luego hacer una prediccion final"

---------------------------------------------------------------------------------------------------------------------------

                                                          METRICAS

##### Regresion lineal

from sklearn.metrics import max_error

max_error()  max_error(y, y_predicho)                          "Calcula el maximo residuo de todos los puntos (solo uno),
                                                                en este caso solo del eje "y""

from sklearn.metrics import mean_absolute_error

mean_absolute_error()  mean_absolute_error(y, y_predicho)      "Calcula la media de todos los residuos de la columna "y"

from sklearn.metrics import mean_squared_error

mean_squared_error()  mean_squared_error(y, y_predicho, squared=True)  "Calcula los residuos al cuadrados sumados y luego
                                                                        los divide por la cantidad, en este caso squared=True
                                                                        es para NO aplicarle raiz cuadrada, False le aplicara
                                                                        la raiz cuadrada"

from sklearn.metrics import r2_score

r2_score()  r2_score(y, y_predicho)                            "Dice que tan bueno es el modelo, 1 significa que el modelo
                                                                es perfecto, muy cercano a 1 es bueno, no tiene limite en
                                                                negativo, ES SOLO PARA REGRESIONES LINEALES"

                                                                r2_score(y, cross_val_predict(...)) si da cercano a 1 es bueno

from sklearn.metrics import accuracy_score

accuracy_score()  accuracy_score(y_prueba, y_prediccion)       "Es la acertividad de una matriz de confusion, a todos los
                                                                demas funcuines se les entrega y_test, y_predict"

from sklearn.metrics import precision_score

from sklearn.metrics import recall_score

from sklearn.metrics import f1_score

from sklearn.metrics import roc_auc_score

from sklearn.metrics import confusion_matrix                   "Hay que darle los y_test y y_predict, dara la matriz, para
                                                                graficarla hay que hacerla con seaborn (heatmap)"

from sklearn.metrics import roc_curve

from sklearn.metrics import precision_recall_curve

---------------------------------------------------------------------------------------------------------------------------

                                                           MODELOS

from sklearn.model_selection import train_test_split

train_test_split()  train_test_split(datos_iniciales, categorias) "Este comando entre 4 arrays continuos, siempre lo que se
                                                                   hace es definir variables para 4 array para que sea, mas
                                                                   ordenado"

datos_entrenamiento, datos_testeo, objetivo_entrenamiento, objetivo_prueba = train_test_split(datos_iniciales, categoiras)

                                                               "Lo que hace es obtener los datos y las categorias que les damos
                                                                (datos de entrenamiento), luego los revuelve y los vuelve a
                                                                ocupar para hacer una prueba y el predice la categoria
                                                                (datos de prueba)"

                                   datos_entrenamiento (x) ->  "Es un array con los mismos datos, pero copiados, al hacer
                                                                la copia los revuelve, no estan en el mismo orden, son los
                                                                datos para entrenar"

                                   datos_testeo  (x_test)  ->  "Es un array con los mismos datos, pero copiados, al hacer
                                                                una copia los revuelve, estos son datos de prueba"

                                   objetivo_entrenamiento (y) -> "Es un array que dice la categoria de cada datos de entrenamiento"

                                   objetivo_prueba  (y_test)  -> "Es un array que dice la categoria de los datos de prueba, hace
                                                                una prediccion"


                                   train_test_split(datos_iniciales, categoiras, test_size=0.3)

                                                               "Por defecto ocupar 75% de los datos para entrenamiento y 
                                                                25% para prueba, pero eso se puede cambiar con test_size
                                                                en este caso 0.3 es 30% y el otro se modifica automaticamente"


from sklearn.model_selection import ShuffleSplit

ShuffleSplit()  ShuffleSplit()                                 "Lo que hace es para un conjunto de datos ya sea numeros,
                                                                string, tuplas(los cuenta como 1 dato), los mezcla y 
                                                                muestra los indices de todas las mezcla, separandolas
                                                                en datos de entramiento y datos de prueba"

                SS = ShuffleSplit(n_splits=5, test_size=.25, random_state=0) 

                                                               "Esto es para poner la operacion en una variable, con
                                                                n_splits es el numero de iteraciones (o mezclas que hara)
                                                                test_size es el porcentaje de datos de prueba que hara,
                                                                lo de entramiento es todo lo que sobre, random_state es una
                                                                especie de plantilla de mezclas, cada valor es una mezcla
                                                                establecida en posiciones, de tal forma que random_state=0
                                                                es siempre la misma mezcla y asi, etc.

                a, b, c, d, e = SS.split(columna_datos)        "Hara arrays dependiendo de las iteraciones que le hayamos
                                                                pruesto en este caso le pusimos 5 iteraciones, hara 5 arrays
                                                                , pero esos array se subdividen en 2, por lo que en total son
                                                                10, pero para copiar en variables los otros array, necesitamos
                                                                hacer un for para darles una variable"

                                    a, b, c, d, e              "Cada una variable contiene 2 arrays, un array son los indices
                                                                de los datos de entrenamiento y el otro infices de los datos
                                                                de prueba, son solo las posiciones de los datos no los datos"
                                                                
                                                                                                                         
  from sklearn.model_selection import KFold

  KFold()  KFold(n_splits=1, random_state=0, shuffle=True)     "Hace lo mismo que ShuffleSplit, hace bloques y las posiciones
                                                                de los datos de la misma forma, la opcion shuffle=True significa
                                                                que mezcla los datos antes de crearlos, al poner shuffle=False
                                                                evitaremos fuga de datos, depende las repeticiones va mezclando
                                                                los datos como validacion cruzada, de esta forma obtendremos
                                                                los array"                                                                                                                                                          


from sklearn.model_selection import cross_val_score

cross_val_score()  cross_val_score(tipo_ajuste, x, y, cv=5)    "Hace una validacion cruzada, recordando que la validacion
                                                                cruzada es solo para confirmar los parametros del modelo
                                                                estan bien, entonces lo que hace es separar en 5 grupos
                                                                (cv=5), estos grupos se iran intercalando los datos de 
                                                                entrenamiento con los datos de validacion (prueba), para
                                                                probar que estan bien, al ponerlo como variable tendremos
                                                                un array con los resuldas, podemos sacarle la media y 
                                                                la desviacion standart"


from sklearn.model_selection import cross_val_predict         

cross_val_predict()  cross_val_predict(funcion, x, y, cv=5)

                                                               "Hace lo mismo que cross_val_score, pero la mezcla de datos
                                                                no es igual, en este caso solo mezcla datos de forma aleatoria
                                                                es como tomar datos al azar por lo que no es correcto para
                                                                hacer una validacion cruzada, pero puede ser util para comparar
                                                                o visualizar datos de modelos"

                                                                "Se le puede aplicar a cv=KFold() para que sea igual a
                                                                cross_val_score"




from sklearn.model_selection import RidgeCV

RidgeCV()  RidgeCV(alphas, cv=4)                               "Es un tipo de regresion que Ridge, pero en este caso ya tiene
                                                                la validacion cruzada, es lo mismo que GridSearch con Ridge"




from sklearn.model_selection import LassoCV

LassoCV()  LassoCV(alphas, cv=4)                               "Es lo mismo que combinar GridSearch con la regresion llamada
                                                                Lasso"

from sklearn.linear_model import LogisticRegressionCV

LogisticRegression()  LogisticRegression(cs=10, cv=4, penalty="ll", solver="liblinear")

                                                               "Es lo mismo que combinar GridSearch con la regresion
                                                                logistica, cs es el valor por defecto"                                                             


from sklearn.model_selection import GridSearchCV

GridSearch()  GridSearch(estimador, hiperparametros, cv=5)     "Encontrara los mejores parametros para ese estimador y para
                                                                esos hiperparametros, generalmente se ocupa para forest y
                                                                esa estimacion, pueden ser diccionarios y tener varios modelos,
                                                                etc"

                                                                Muy util para encontrar en una regresion polinomica que
                                                                grado queda mejor para una regresion o en caso de regresiones
                                                                ridge o lasso los mejores parametros beta para esos modelos,
                                                                porque se pueden probar varios a la vez

                                                                grid = GridSearch(...)

                                                                pd.DataFrame(grid.cv_results_) #muestra como dataframe donde
                                                                se le a√±aden nuevas columnas con puntuaciones

                                                                grid.fit(x,y)

                                                                grid.best_score_ , grid.best_params_   #Muestras los mejores
                                                                                                        hiperparametros 


from sklear.model_selection import StratifiedShuffleSplit

StratifiedShuffleSplit()  StratifiedShuffleSplit(n_splits=1, test_size=0.3, random_state=)

                                                               "Es muy parecido a los anteriores con los split significan
                                                                la cantidad de grupos que hara, pero en este caso la unica
                                                                diferencia es que en cada division de entrenamiento y las 
                                                                de prueba, estan dividias por porcentajes de categorias"

---------------------------------------------------------------------------------------------------------------------------

                                                   CARACTERISTICAS

from sklearn.feature_selection import RFE

RFE()  refMod = RFE(est, n_features_to_select=5)              "n_features_to_select=5 son la cantidad de caracteristicas
                                                               que queremos conservar 

                             rfeMod.fit(X_train, y_train)
                             y_predict = rfeMod.predict(X_test)                                                            

                                                                                            


---------------------------------------------------------------------------------------------------------------------------

                                                      PIPELINE

from sklearn.pipeline import Pipeline

Pipeline()  pipe = Pipeline([("nombre_operacion1", operacion1), ("nombre_operacion2", operacion2)])

            pipe.fit(x_train,y_train)   pipe.funcion() #a√±ade otra funcion adicional

                                                               "Lo que hace es que puede hacer varias operaciones o funciones
                                                                a la vez, entendiendo por orden que primero se hace la primera
                                                                segundo la segunda, generalmente se ocupa un escalador y luego
                                                                un modelo de ajuste a eso se le llama estimador"

                                                                cross_val_predict(pipe, x, y, cv=KFold(....))

                                                                Para esto es bueno ocupar una pipeline

