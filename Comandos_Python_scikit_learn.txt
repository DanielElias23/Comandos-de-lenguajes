











---------------------------------------------------------------------------------------------------------------------------
                                                 MACHING LEARNING SUPERVISADO
---------------------------------------------------------------------------------------------------------------------------

                                                     COMANDOS GENERALES




--------------------------------------------------------------------------------------------------------------------------
        
                                                      PREPROCESAMIENTO

Para ocupar cada funcion como son muy largos los nombres se pasan a variables y luego para hacer efectivos los cambios
se pasan a array con un transformador

Ejemplo: OHE = OneHotEncoder()
         OHE.fit_transform(datos)   #OHE se queda con los datos que le pasaron, estos datos estaran por defecto

OHE.transform(columna)  ##transform es para aplicar la transformacion a la columna (codificacion en este caso)

OHE.inverse_transform(resulado_anterior)   ##deshace el cambio anterior, vuelve a la normalidad

OHE.classes_   ##muestra la cantidad de datos uncicos dentro del fit

####         


from sklearn.preproccessing import OneHotEncoder

OneHotEncoder()  OneHotEncoder()                               "Una columna de catergoria con string, la divide en varias columnas
                                                                dependiendo de la cantidad de categorias, ejemplo 3 categorias
                                                                las divide en 3 columna, siendo el nombre de la columna la
                                                                categoria anterior, si la categoria de esa FILA es una la
                                                                marcara con 1 en esa columna y 0 cuando no lo es, en todas las
                                                                otras categorias marcara 0, ejemplo: [0, 1, 0] o [1, 0, 0]"

from sklearn.preproccessing import LabelsEncoder

LabelsEncoder()  LabelsEncoder()                               "Una columna con categorias con string, simplemente las
                                                                cambia de string a valor numerico, asignandole un numero
                                                                entero a cada categoria, ejemplo: "Gris"  a valor 1,
                                                                "Verde" a valor 2, etc y crea una nueva columna"

from sklearn.preproccessing import MinMaxScaler

MinMaxScaler()  MinMaxScaler()                                 "Lo que hace es escalar los valores numericos entre 0 y 1,
                                                                siendo por ejemplo una columna con valores, el mayor
                                                                valor de esa columna, sera 1 y el menor sera 0, todos los otros
                                                                estaran en la misma relacion unos a otros, pero seran valores
                                                                entre 0 y 1, hay una formula para esto"

from sklearn.preproccessing import Scale

Scale()  Scale(X, axis=0)                                      "Escala los datos al parecer de forma aleatoria en cuanto
                                                                relacion entre cada valor, pero mantienen una proporcion
                                                                entre todos los datos, siguen una mediana y una varianza
                                                                dependiendo del valor que le entregemos, axis=0 significa
                                                                escala por separado por columna, axis=1 significa que escala
                                                                por fila"


from sklearn.preproccessing import StandardScaler

StandardScaler()  StandardScaler()                             "Traspasa todos los datos a la otra escala, los datos son
                                                                modificados para que sean valores oscilando cerca de 0,
                                                                pero la relacion entre todos se mantiene, al graficarlos
                                                                se conserva la misma forma"

from sklearn.preproccessing import PolynomialFeatures

PolynomialFeatures()  PolynomialFeatures(degree=2, include_bias=True)
                                       
                                                               "Se dice que les crea mas categorias a los datos, pero lo que
                                                                es crea un nuevo array con valores que se le aplican ciertas
                                                                operaciones matematicas (Se le llama polynomica) es como si
                                                                expandiera el dato en mas datos"

                      pf = PolynomialFeatures(degree=2, include_bias=True)
                      pf.fit.transform([2,3])
                                                               "Lo que hace es [2,3], como le dijimos bias si agrega un valor
                                                                1, entonces lo que hace es crea una nueva tupla
                                                                con en este caso 6 valores porque es grado dos, hace las
                                                                siguientes operaciones: [1, 2^1, 3^1, 2^2, 3^2, 2*3] y asi
                                                                sucesivamente en caso de agregar mas grados"

                      X_poly = pf.fit.transform(data[["x"]])   "Para ocupar caracteristicas polinomiales a una columna
                                                                tenemos que ponerla en doble corchete"

                      lr = LinearRegresion.fit(X_poly, Y_data) 

                      Y_pred = lr.predict(X_poly)                                                               


from sklearn.preproccessing import LabelBinarizer

LabelBinarizer()  LabelBinarizer()                             "Genera una matriz en binario, desde una columna en el fit
                                                                lo que hara [2,3] dentro del fit entendera como que solo
                                                                quieres mostrar la columna 2 y 3, el resultado seran dos
                                                                columna, matriz de nx2, en el transform va otra tupla, con
                                                                [2,4] que cada valor indica donde van los 1, todo esta con 0,
                                                                excepto las columnas [2,4], la 4 en este caso no se muestra,
                                                                por que en el fit no se le especifico.

                                                                [[1,0]  Ira poniendo los 1 fila por fila, fila 1, columna 2,
                                                                [0,0]]  colocara un 1, (la columna 2 es la que se muestra
                                                                        de las primeras, la cantidad de filas que tenga
                                                                        sera debido a la cantidad de datos en transform)

from sklearn.preproccessing import OrdinalEncoder
                                                                        
OrdinalEncoder() OrdinalEncoder()                              "Al entregarle una tupla con subtuplas [[,],[,]] al fit
                                                                lo que hara sera darle un numero a cada dato, puede ser string
                                                                0 al primero que encuentre en caso de string, 0 al numero mas
                                                                bajo que encuentre, diferenciara automaticamente si son numeros
                                                                o string, creado dos codificaciones simultaneras para cada tipo
                                                                de dato, lo que hara sera entregar la misma tupla pero solo con
                                                                numeros que categorizan y se refieren a una categoria. Crea
                                                                una matriz solo con los datos que le especifiquemos en el
                                                                transform([[,],[,]]), pero la categorizacion se creo en el 
                                                                fit.


from sklearn.preproccessing import FuntionTransformer

FuntionTransformer()  FuntionTransformer(np.funcion) funcion como np.log1p sin parentesis

                                                              "Para aplicarle una funcion a todos los datos, es un escalamieto
                                                               personalizado, en este caso se le aplica ln(1+x) a todos los
                                                               datos, luego se aplica FuntionTransformer.transform(X) con
                                                               esto se hace efectivo el cambio"



---------------------------------------------------------------------------------------------------------------------------

                                                  REGRESIONES Y CLASIFICADORES

from sklearn.linear_model import LinearRegression    


LinearRegression()  clf = LinearRegression()                   "Crea un modelo para regresion lineal (le pasa las formulas)"

fit()  clf.fit(x,y)                                            "Entrena el modelo creado (hace una regresion lineal)"

coef_  clf.coef_                                               "Nos entrega el valor "b0" para esos datos"

intercept_  clf.intercept_                                     "Nos entrega el valor "b1" para esos datos"

predict()  clf.predict([[x]])  clf.predict(x) (para una columna entera)

                                                               "Segun nuestro modelo nos predice un valor para ese x,
                                                                todo esto clf.predict(x) = y_predecido"


from sklearn.linear_model import Ridge

Ridge()  Ridge(columna, max_iter=100, alpha=0.001)             "Es un otro tipo de regresion que tiene penalizacion por
                                                                por un parametro beta y otro lamnda, lo que voy a buscar
                                                                en esta regresion es que los parametros sean pequeños, se
                                                                puede minimizar, alpha defina el valor del parametro"


from sklearn.linear_model import Lasso

Lasso()  Lasso(columna, max_iter=100)                          "Es otra regresion, pero esta no se puede minimizar con
                                                                derivada, hay que probar por intento, el beneficio es
                                                                que se puede anular un parametro en la minimizacion"


from sklearn.linear_model import ElasticNetCV

ElasticNetCV()  ElasticNet(alphas, ll_ratio, max_iter=1000).fit(X_train, y_train)

                                                               "Es una mezcla de las regresiones Ridge y Lasso, ademas con
                                                                GridSearch, alphas y ll_ratio son los coeficientes,
                                                                pueden ser listas de datos"

from sklearn.linear_model import LogisticRegression

LogisticRegression()  LogisticRegression(penalty="l2", c=10, solver="liblinear")

                                                               "Hace la regresion logistica, con penality es para penalizar
                                                                con el agregado de lasso y el valor de la constante de
                                                                regulacion 10, "c" mas alta menos penalizacion"                                                                

from sklearn.neighbors import KNeighborsClassifier

KNeighborsClassifier()  KNeighborsClassifier(n_neighbors=3)    "Para hacer una classificacion KNN donde n_neghbors es k,
                                                                tambien posteriormente ocupa fit y prefict"

from sklean.svm import LinearSVC

LinearSVC() LinearSVC(penalty="12", C=10)                      "Es para ocupar la clasificacion support vector machine,
                                                                donde le especificamos la regulacion de los coeficientes,
                                                                tenemos que entregarle un fit(x_train, y_train)

from sklearn.svm import SVC

SVC()  SCV(kernel="rbf", gamma=1, C=10)                        "Es para ocupar la clasificacion support vector machine, pero
                                                                en este caso la version no lineal, ocupando nucleos, gamma
                                                                y C son coeficientes de regulacion, valores bajos para ambos
                                                                es mas regulacion y valores, mas altos menos regulacion"

from sklearn.kernel_approximation import Nystroem

Nystroem()  Nystroem(kernel="rbf", gamma=1, C=10)

from sklearn.kernel_approximation import RBFsampler

RBFsampler()  RBFsampler(gamma=1, n_components=100)


from sklearn.tree import DecisionTreeClassifier

DecisionTreeClassifier()  DecisionTreeClassifier(criterion="Gini", max_features=10, max_depth=5, random_state=0)

                                                              "Para ocupar un arbol de decision con criterio y error de Gini
                                                               con profundidad 5, se le entregan los datos de entrenamiento
                                                               al fit(x_train, y_train)"

tree_  DecisionTreeClassifier.tree_                           "Es para ver los datos del arbol que estamos estudiando"

node_count DecisionTreeClassifier.tree_.node_count            "Muestra la cantidad de nodos del arbol"

max_depth  DecisionTreeClassifier.tree_.max_depth             "Dice la profundidad del arbol"



from sklearn.tree import DecisionTreeRegressor

DecisionTreeRegressor()  DecisionTreeRegressor()              "Es para hacer regresiones con arbol de deciciones, tiene que
                                                               tener un fit"

from sklearn.tree import export_graphvis
from io import StringIO
import pydotplus


dot_data=StringIO()

export_graphvis()  export_graphvis(DecisionTreeClassifier(), out_file=dot_data, filled=True)

                                                               "Para graficar los arboles de decision"

from sklearn.ensemble import BaggingClassifier

BaggingClassifier()  BaggingClassifier(n_estimators=50)        "Es para hacer un empaquetado de arboles de decicion, donde
                                                                el numero de estimadores, es el numero de arboles que tendremos,
                                                                necesita fit(x_train, y_train)"

from sklearn.ensemble import BaggingRegressor

from sklearn.ensemble import RandomForestClassifier

RandomForestClassifier()  RandomForestClassifier(n_estimators=50)

from sklearn.ensemble import ExtraTreeClassifier

ExtraTreeClassifier()  ExtraTreeClassifier(n_estimators=50)

from sklearn.ensemble import RandomForestRegressor

from sklearn.ensemble import GradientBoostingClassifier

GradientBoostingClassifier()  GradientBoostingClassifier(learning_rate=0.1, max_features=1, subsample=0.5, n_estimators=200)

                                                               "Al gradiente le aplican fit(x_train, y_train) luego predict"

from sklearn.ensemble import AdaBoostClassifier

AdaBoostClassifier()  AdaBoostClassifier(base_estimators=DecisionTreeClassifier(), learning_rate=0.1, n_estimators=200)

                                                               "A adaboost se le aplicar el fit y luego predict"

from sklearn.ensemble import VotingClassifier

VotingClassifier() VotingClassifier(lista_de_estimadores)      "Esta funcion es para apilar (stacking) clasificadores,
                                                                como bosques aleatorios, regresion logistica, etc, luego
                                                                se ocupar fit(x_train, y_train) y predict"

from sklearn.ensemble import StackingClassifier

StackingClassifier()  StackingClassifier(lista_de_estimadores, final_estimator=LogisticRegression())                    

                                                               "Es similar al anterior, permite ocupar diferentes
                                                                clasificadores, votar y luego hacer una prediccion final"


from sklearn.cluster import KMeans

KMeans()  KMeans(n_cluster=3, init="k-means++")                "Este codigo empieza iterar n_cluster sera el resultado
                                                                final de cluster, k-means es la distancia total al cuadrado,
                                                                luego hay que ocupar fit(X1) y predict"

                                 inertia=[]
                                 list_cluster=list(1,2,3,4,5,6,7,8,9,10)
                                 for k in list_cluster:
                                      kmeans=KMeans(n_cluster=k)
                                      kmeans.fit(X)
                                      inertia.append(km.inertia_)

                                                               "forma de itinerar los diferentes k y obtener la inertia"


 from sklearn.cluster import MiniBatchKMeans

 MiniBatchKMeans()  MiniBatchKMeans()                          "En este caso toma lotes aleatorios, pero mas pequeños que el
                                                                comando anterior, esto hace que sea mas rapido"                                                               



---------------------------------------------------------------------------------------------------------------------------

                                                          METRICAS

##### Regresion lineal

from sklearn.metrics import max_error

max_error()  max_error(y, y_predicho)                          "Calcula el maximo residuo de todos los puntos (solo uno),
                                                                en este caso solo del eje "y""

from sklearn.metrics import mean_absolute_error

mean_absolute_error()  mean_absolute_error(y, y_predicho)      "Calcula la media de todos los residuos de la columna "y"

from sklearn.metrics import mean_squared_error

mean_squared_error()  mean_squared_error(y, y_predicho, squared=True)  "Calcula los residuos al cuadrados sumados y luego
                                                                        los divide por la cantidad, en este caso squared=True
                                                                        es para NO aplicarle raiz cuadrada, False le aplicara
                                                                        la raiz cuadrada"

from sklearn.metrics import r2_score

r2_score()  r2_score(y, y_predicho)                            "Dice que tan bueno es el modelo, 1 significa que el modelo
                                                                es perfecto, muy cercano a 1 es bueno, no tiene limite en
                                                                negativo, ES SOLO PARA REGRESIONES LINEALES"

                                                                r2_score(y, cross_val_predict(...)) si da cercano a 1 es bueno

from sklearn.metrics import accuracy_score

accuracy_score()  accuracy_score(y_prueba, y_prediccion)       "Es la acertividad de una matriz de confusion, a todos los
                                                                demas funcuines se les entrega y_test, y_predict"

from sklearn.metrics import precision_score

from sklearn.metrics import recall_score

from sklearn.metrics import f1_score

from sklearn.metrics import roc_auc_score

from sklearn.metrics import confusion_matrix                   "Hay que darle los y_test y y_predict, dara la matriz, para
                                                                graficarla hay que hacerla con seaborn (heatmap)"

from sklearn.metrics import roc_curve

from sklearn.metrics import precision_recall_curve

---------------------------------------------------------------------------------------------------------------------------

                                                           MODELOS

from sklearn.model_selection import train_test_split

train_test_split()  train_test_split(datos_iniciales, categorias) "Este comando entre 4 arrays continuos, siempre lo que se
                                                                   hace es definir variables para 4 array para que sea, mas
                                                                   ordenado"

datos_entrenamiento, datos_testeo, objetivo_entrenamiento, objetivo_prueba = train_test_split(datos_iniciales, categoiras)

                                                               "Lo que hace es obtener los datos y las categorias que les damos
                                                                (datos de entrenamiento), luego los revuelve y los vuelve a
                                                                ocupar para hacer una prueba y el predice la categoria
                                                                (datos de prueba)"

                                   datos_entrenamiento (x) ->  "Es un array con los mismos datos, pero copiados, al hacer
                                                                la copia los revuelve, no estan en el mismo orden, son los
                                                                datos para entrenar"

                                   datos_testeo  (x_test)  ->  "Es un array con los mismos datos, pero copiados, al hacer
                                                                una copia los revuelve, estos son datos de prueba"

                                   objetivo_entrenamiento (y) -> "Es un array que dice la categoria de cada datos de entrenamiento"

                                   objetivo_prueba  (y_test)  -> "Es un array que dice la categoria de los datos de prueba, hace
                                                                una prediccion"


                                   train_test_split(datos_iniciales, categoiras, test_size=0.3)

                                                               "Por defecto ocupar 75% de los datos para entrenamiento y 
                                                                25% para prueba, pero eso se puede cambiar con test_size
                                                                en este caso 0.3 es 30% y el otro se modifica automaticamente"


from sklearn.model_selection import ShuffleSplit

ShuffleSplit()  ShuffleSplit()                                 "Lo que hace es para un conjunto de datos ya sea numeros,
                                                                string, tuplas(los cuenta como 1 dato), los mezcla y 
                                                                muestra los indices de todas las mezcla, separandolas
                                                                en datos de entramiento y datos de prueba"

                SS = ShuffleSplit(n_splits=5, test_size=.25, random_state=0) 

                                                               "Esto es para poner la operacion en una variable, con
                                                                n_splits es el numero de iteraciones (o mezclas que hara)
                                                                test_size es el porcentaje de datos de prueba que hara,
                                                                lo de entramiento es todo lo que sobre, random_state es una
                                                                especie de plantilla de mezclas, cada valor es una mezcla
                                                                establecida en posiciones, de tal forma que random_state=0
                                                                es siempre la misma mezcla y asi, etc.

                a, b, c, d, e = SS.split(columna_datos)        "Hara arrays dependiendo de las iteraciones que le hayamos
                                                                pruesto en este caso le pusimos 5 iteraciones, hara 5 arrays
                                                                , pero esos array se subdividen en 2, por lo que en total son
                                                                10, pero para copiar en variables los otros array, necesitamos
                                                                hacer un for para darles una variable"

                                    a, b, c, d, e              "Cada una variable contiene 2 arrays, un array son los indices
                                                                de los datos de entrenamiento y el otro infices de los datos
                                                                de prueba, son solo las posiciones de los datos no los datos"
                                                                
                                                                                                                         
  from sklearn.model_selection import KFold

  KFold()  KFold(n_splits=1, random_state=0, shuffle=True)     "Hace lo mismo que ShuffleSplit, hace bloques y las posiciones
                                                                de los datos de la misma forma, la opcion shuffle=True significa
                                                                que mezcla los datos antes de crearlos, al poner shuffle=False
                                                                evitaremos fuga de datos, depende las repeticiones va mezclando
                                                                los datos como validacion cruzada, de esta forma obtendremos
                                                                los array"                                                                                                                                                          


from sklearn.model_selection import cross_val_score

cross_val_score()  cross_val_score(tipo_ajuste, x, y, cv=5)    "Hace una validacion cruzada, recordando que la validacion
                                                                cruzada es solo para confirmar los parametros del modelo
                                                                estan bien, entonces lo que hace es separar en 5 grupos
                                                                (cv=5), estos grupos se iran intercalando los datos de 
                                                                entrenamiento con los datos de validacion (prueba), para
                                                                probar que estan bien, al ponerlo como variable tendremos
                                                                un array con los resuldas, podemos sacarle la media y 
                                                                la desviacion standart"


from sklearn.model_selection import cross_val_predict         

cross_val_predict()  cross_val_predict(funcion, x, y, cv=5)

                                                               "Hace lo mismo que cross_val_score, pero la mezcla de datos
                                                                no es igual, en este caso solo mezcla datos de forma aleatoria
                                                                es como tomar datos al azar por lo que no es correcto para
                                                                hacer una validacion cruzada, pero puede ser util para comparar
                                                                o visualizar datos de modelos"

                                                                "Se le puede aplicar a cv=KFold() para que sea igual a
                                                                cross_val_score"




from sklearn.model_selection import RidgeCV

RidgeCV()  RidgeCV(alphas, cv=4)                               "Es un tipo de regresion que Ridge, pero en este caso ya tiene
                                                                la validacion cruzada, es lo mismo que GridSearch con Ridge"




from sklearn.model_selection import LassoCV

LassoCV()  LassoCV(alphas, cv=4)                               "Es lo mismo que combinar GridSearch con la regresion llamada
                                                                Lasso"

from sklearn.linear_model import LogisticRegressionCV

LogisticRegression()  LogisticRegression(cs=10, cv=4, penalty="ll", solver="liblinear")

                                                               "Es lo mismo que combinar GridSearch con la regresion
                                                                logistica, cs es el valor por defecto"                                                             


from sklearn.model_selection import GridSearchCV

GridSearch()  GridSearch(estimador, hiperparametros, cv=5)     "Encontrara los mejores parametros para ese estimador y para
                                                                esos hiperparametros, generalmente se ocupa para forest y
                                                                esa estimacion, pueden ser diccionarios y tener varios modelos,
                                                                etc"

                                                                Muy util para encontrar en una regresion polinomica que
                                                                grado queda mejor para una regresion o en caso de regresiones
                                                                ridge o lasso los mejores parametros beta para esos modelos,
                                                                porque se pueden probar varios a la vez

                                                                grid = GridSearch(...)

                                                                pd.DataFrame(grid.cv_results_) #muestra como dataframe donde
                                                                se le añaden nuevas columnas con puntuaciones

                                                                grid.fit(x,y)

                                                                grid.best_score_ , grid.best_params_   #Muestras los mejores
                                                                                                        hiperparametros 


from sklear.model_selection import StratifiedShuffleSplit

StratifiedShuffleSplit()  StratifiedShuffleSplit(n_splits=1, test_size=0.3, random_state=)

                                                               "Es muy parecido a los anteriores con los split significan
                                                                la cantidad de grupos que hara, pero en este caso la unica
                                                                diferencia es que en cada division de entrenamiento y las 
                                                                de prueba, estan dividias por porcentajes de categorias"4

---------------------------------------------------------------------------------------------------------------------------
                                                REDES NEURONALES
---------------------------------------------------------------------------------------------------------------------------

from sklearn.neural_network import MLPClassifier

MLPClassifier()  MLPClassifier(hidden_layer_sizes=(5,3), activation="logistic")

                                                               "Sklearn solo tiene un modelo simple de redes neuronales, no
                                                                es el mas optimo, hidden_layer_sizes dice cuanta seran las
                                                                capas ocultas, como hay 2 valores son 2 capas, una con 5 nodos
                                                                y la otra con 3 nodos, con la regresion logistica en cada nodo,
                                                                luego le aplicamos un fit(X_train, y_train) y despues
                                                                predict(X_test)"





---------------------------------------------------------------------------------------------------------------------------

                                                     OTROS

from sklearn.feature_selection import RFE

RFE()  refMod = RFE(est, n_features_to_select=5)              "n_features_to_select=5 son la cantidad de caracteristicas
                                                               que queremos conservar 

                             rfeMod.fit(X_train, y_train)
                             y_predict = rfeMod.predict(X_test)                                                            

                                                                                            
from sklearn.utils import shuffle  

shuffle()  shuffle(array)                                     "Revuelve los valores, por defecto al parecer los invierte"


from sklearn.cluster import AgglomerativeClustering

AgglomerativeClustering()  AgglomerativeClustering(n_clusters=3, affinity="euclidean", linkage="ward")

                                                              "Para ocupar agrupaciones aglomerativas, se le ponen el numero
                                                               de cluster, afinidad la forma que se calcula la distancia,
                                                               y el tipo de enlace entre cluster, luego se ocupa fit(X)
                                                               y predict(X2)"


from sklearn.cluster import DBSCAN

DBSCAN()  DBSCAN(eps=3, min_samples=2)                        "Para hacer conglomerados con DBSCAN, eps es epsilon, y
                                                               min_samples es n_clu, luego se ocupa fit(X), luego
                                                               DBSCAN.labels_, en este ultimo entrega valores, 0 pertenece 
                                                               al primer cluster, 1 al segundo cluster, -1 son los outliers"

from sklearn.cluster import MeanShift

MeanShift()  MeanShift(bandwidth=2)                           "Para ocupar el metodo MeanShift donde bandwidth es el tamaño
                                                               de la ventana, despues el fit(X) y predict(X2)

from scipy.cluster import hierarchy

hierarchy()  Z = hierarchy.linkage(AgglomerativeClustering.children_, method="ward")

                 fig, ax = plt.subplots(figsize=(5,5))

                 hierarchy.dendrogram(Z, orientation="top", ax=ax, p=1, truncate_mode="level")

                                                              "Muestra un mapa conceptual jerarquico de los cluster, truncate_mode
                                                               significa que sera cortado el mapa conceptual, p=1 dice cuantos
                                                               niveles mostrara"


from sklearn.decomposition import PCA

PCA()  PCA(n_components=3)                                    "Lo que hacemos es ocupar PCA y nos dice que si tenemos 10
                                                               columnas lo queremos reducir a 3 columnas, luego aplicar
                                                               fit_trasform(X_train)"

                                                       
from sklearn.decomposition import KernelPCA

KernelPCA()  KernelPCA(n_components=3, kernel="rbf", gamma=1)

                                                              "Para ocupar Kernel PCA donde n_componentes son las Dimenciones
                                                               final que queremos, en kernel le ponemos maquina de vectores
                                                               de soporte, y gamma=1 es para decir que tan lineal o no lineal
                                                               quieres el conjunto, luego se le aplica fit_transform(X) o 
                                                               fit(X_train) luego tranform(X_test)

from sklearn.decomposition import MDS

MDS()  MDS(n_components=2)                                    "Para ocupar el escalado multi-dimensional, solo ponemos
                                                               en n_componentes=2 como el numero de dimensiones finales,
                                                               en este caso reduce dimensiones por ejemplo de 3 a 2,
                                                               ocupamos fit_transform(X)"

       MDS(dissimilary="precomputed", random_state=0, n_components=2, metric=False)   

                                                              "Para hacer una MDS sin metrica"                                                            


from sklearn.decomposition import NMF

NMF()  NMF(n_components=3, init="random")                     "Para ocupar NMF, n_components no dice cuantos componentes
                                                               diferentes queremos realmente, y luego init para iniciar
                                                               de forma aleatoria, pero no es necesario, luego solo ocupamos
                                                               fit(X)"


---------------------------------------------------------------------------------------------------------------------------

                                                      PIPELINE

from sklearn.pipeline import Pipeline

Pipeline()  pipe = Pipeline([("nombre_operacion1", operacion1), ("nombre_operacion2", operacion2)])

            pipe.fit(x_train,y_train)   pipe.funcion() #añade otra funcion adicional

                                                               "Lo que hace es que puede hacer varias operaciones o funciones
                                                                a la vez, entendiendo por orden que primero se hace la primera
                                                                segundo la segunda, generalmente se ocupa un escalador y luego
                                                                un modelo de ajuste a eso se le llama estimador"

                                                                cross_val_predict(pipe, x, y, cv=KFold(....))

                                                                Para esto es bueno ocupar una pipeline

