











---------------------------------------------------------------------------------------------------------------------------
                                                 MACHING LEARNING SUPERVISADO
---------------------------------------------------------------------------------------------------------------------------

                                                     COMANDOS GENERALES




--------------------------------------------------------------------------------------------------------------------------
        
                                                      PREPROCESAMIENTO

Para ocupar cada funcion como son muy largos los nombres se pasan a variables y luego para hacer efectivos los cambios
se pasan a array con un transformador

Ejemplo: OHE = OneHotEncoder()
         OHE.fit_transform(datos)

####         


from sklearn.preproccessing import OneHotEncoder

OneHotEncoder()  OneHotEncoder()                               "Una columna de catergoria con string, la divide en varias columnas
                                                                dependiendo de la cantidad de categorias, ejemplo 3 categorias
                                                                las divide en 3 columna, siendo el nombre de la columna la
                                                                categoria anterior, si la categoria de esa FILA es una la
                                                                marcara con 1 en esa columna y 0 cuando no lo es, en todas las
                                                                otras categorias marcara 0, ejemplo: [0, 1, 0] o [1, 0, 0]"

from sklearn.preproccessing import LabelsEncoder

LabelsEncoder()  LabelsEncoder()                               "Una columna con categorias con string, simplemente las
                                                                cambia de string a valor numerico, asignandole un numero
                                                                entero a cada categoria, ejemplo: "Gris"  a valor 1,
                                                                "Verde" a valor 2, etc y crea una nueva columna"

from sklearn.preproccessing import MinMaxScaler

MinMaxScaler()  MinMaxScaler()                                 "Lo que hace es escalar los valores numericos entre 0 y 1,
                                                                siendo por ejemplo una columna con valores, el mayor
                                                                valor de esa columna, sera 1 y el menor sera 0, todos los otros
                                                                estaran en la misma relacion unos a otros, pero seran valores
                                                                entre 0 y 1, hay una formula para esto"


from sklearn.preproccessing import StandardScaler

StandardScaler()  StandardScaler()                             "Traspasa todos los datos a la otra escala, los datos son
                                                                modificados para que sean valores oscilando cerca de 0,
                                                                pero la relacion entre todos se mantiene, al graficarlos
                                                                se conserva la misma forma"

from sklearn.preproccessing import PolynomialFeatures

PolynomialFeatures()  PolynomialFeatures(degree=2, include_bias=True)
                                       
                                                               "Se dice que les crea mas categorias a los datos, pero lo que
                                                                es crea un nuevo array con valores que se le aplican ciertas
                                                                operaciones matematicas (Se le llama polynomica) es como si
                                                                expandiera el dato en mas datos"

                      pf = PolynomialFeatures(degree=2, include_bias=True)
                      pf.fit.transform([2,3])
                                                               "Lo que hace es [2,3], como le dijimos bias si agrega un valor
                                                                1, entonces lo que hace es crea una nueva tupla
                                                                con en este caso 6 valores porque es grado dos, hace las
                                                                siguientes operaciones: [1, 2^1, 3^1, 2^2, 3^2, 2*3] y asi
                                                                sucesivamente en caso de agregar mas grados"



   

---------------------------------------------------------------------------------------------------------------------------

                                                        REGRESIONES

from sklearn.linear_model import LinearRegression    


LinearRegression()  clf = LinearRegression()                   "Crea un modelo para regresion lineal (le pasa las formulas)"

fit()  clf.fit(x,y)                                            "Entrena el modelo creado (hace una regresion lineal)"

coef_  clf.coef_                                               "Nos entrega el valor "b0" para esos datos"

intercept_  clf.intercept_                                     "Nos entrega el valor "b1" para esos datos"

predict()  clf.predict([[x]])  clf.predict(x) (para una columna entera)

                                                               "Segun nuestro modelo nos predice un valor para ese x,
                                                                todo esto clf.predict(x) = y_predecido"


---------------------------------------------------------------------------------------------------------------------------

                                                          METRICAS

##### Regresion lineal

from sklearn.metrics import max_error

max_error()  max_error(y, y_predicho)                          "Calcula el maximo residuo de todos los puntos (solo uno),
                                                                en este caso solo del eje "y""

from sklearn.metrics import mean_absolute_error

mean_absolute_error()  mean_absolute_error(y, y_predicho)      "Calcula la media de todos los residuos de la columna "y"

from sklearn.metrics import mean_squared_error

mean_squared_error()  mean_squared_error(y, y_predicho, squared=True)  "Calcula los residuos al cuadrados sumados y luego
                                                                        los divide por la cantidad, en este caso squared=True
                                                                        es para NO aplicarle raiz cuadrada, False le aplicara
                                                                        la raiz cuadrada"

from sklearn.metrics import r2_score

r2_score()  r2_score(y, y_predicho)                            "Dice que tan bueno es el modelo, 1 significa que el modelo
                                                                es perfecto, muy cercano a 1 es bueno, no tiene limite en
                                                                negativo, ES SOLO PARA REGRESIONES LINEALES"

                                                                r2_score(y, cross_val_predict(...)) si da cercano a 1 es bueno



---------------------------------------------------------------------------------------------------------------------------

                                                           MODELOS

from sklearn.model_selection import train_test_split

train_test_split()  train_test_split(datos_iniciales, categorias) "Este comando entre 4 arrays continuos, siempre lo que se
                                                                   hace es definir variables para 4 array para que sea, mas
                                                                   ordenado"

datos_entrenamiento, datos_testeo, objetivo_entrenamiento, objetivo_prueba = train_test_split(datos_iniciales, categoiras)

                                                               "Lo que hace es obtener los datos y las categorias que les damos
                                                                (datos de entrenamiento), luego los revuelve y los vuelve a
                                                                ocupar para hacer una prueba y el predice la categoria
                                                                (datos de prueba)"

                                   datos_entrenamiento (x) ->  "Es un array con los mismos datos, pero copiados, al hacer
                                                                la copia los revuelve, no estan en el mismo orden, son los
                                                                datos para entrenar"

                                   datos_testeo  (x_test)  ->  "Es un array con los mismos datos, pero copiados, al hacer
                                                                una copia los revuelve, estos son datos de prueba"

                                   objetivo_entrenamiento (y) -> "Es un array que dice la categoria de cada datos de entrenamiento"

                                   objetivo_prueba  (y_test)  -> "Es un array que dice la categoria de los datos de prueba, hace
                                                                una prediccion"


                                   train_test_split(datos_iniciales, categoiras, test_size=0.3)

                                                               "Por defecto ocupar 75% de los datos para entrenamiento y 
                                                                25% para prueba, pero eso se puede cambiar con test_size
                                                                en este caso 0.3 es 30% y el otro se modifica automaticamente"


from sklearn.model_selection import ShuffleSplit

ShuffleSplit()  ShuffleSplit()                                 "Lo que hace es para un conjunto de datos ya sea numeros,
                                                                string, tuplas(los cuenta como 1 dato), los mezcla y 
                                                                muestra los indices de todas las mezcla, separandolas
                                                                en datos de entramiento y datos de prueba"

                SS = ShuffleSplit(n_splits=5, test_size=.25, random_state=0) 

                                                               "Esto es para poner la operacion en una variable, con
                                                                n_splits es el numero de iteraciones (o mezclas que hara)
                                                                test_size es el porcentaje de datos de prueba que hara,
                                                                lo de entramiento es todo lo que sobre, random_state es una
                                                                especie de plantilla de mezclas, cada valor es una mezcla
                                                                establecida en posiciones, de tal forma que random_state=0
                                                                es siempre la misma mezcla y asi, etc.

                a, b, c, d, e = SS.split(columna_datos)        "Hara arrays dependiendo de las iteraciones que le hayamos
                                                                pruesto en este caso le pusimos 5 iteraciones, hara 5 arrays
                                                                , pero esos array se subdividen en 2, por lo que en total son
                                                                10, pero para copiar en variables los otros array, necesitamos
                                                                hacer un for para darles una variable"

                                    a, b, c, d, e              "Cada una variable contiene 2 arrays, un array son los indices
                                                                de los datos de entrenamiento y el otro infices de los datos
                                                                de prueba, son solo las posiciones de los datos no los datos"
                                                                
                                                                                                                         
  from sklearn.model_selection import KFold

  KFold()  KFold(n_splits=1, random_state=0, shuffle=True)     "Hace lo mismo que ShuffleSplit, hace bloques y las posiciones
                                                                de los datos de la misma forma, la opcion shuffle=True significa
                                                                que mezcla los datos antes de crearlos, al poner shuffle=False
                                                                evitaremos fuga de datos, depende las repeticiones va mezclando
                                                                los datos como validacion cruzada, de esta forma obtendremos
                                                                los array"                                                                                                                                                          


from sklearn.model_selection import cross_val_score

cross_val_score()  cross_val_score(tipo_ajuste, x, y, cv=5)    "Hace una validacion cruzada, recordando que la validacion
                                                                cruzada es solo para confirmar los parametros del modelo
                                                                estan bien, entonces lo que hace es separar en 5 grupos
                                                                (cv=5), estos grupos se iran intercalando los datos de 
                                                                entrenamiento con los datos de validacion (prueba), para
                                                                probar que estan bien, al ponerlo como variable tendremos
                                                                un array con los resuldas, podemos sacarle la media y 
                                                                la desviacion standart"


from sklearn.model_selection import cross_val_predict         

cross_val_predict()  cross_val_predict(funcion, x, y, cv=5)

                                                               "Hace lo mismo que cross_val_score, pero la mezcla de datos
                                                                no es igual, en este caso solo mezcla datos de forma aleatoria
                                                                es como tomar datos al azar por lo que no es correcto para
                                                                hacer una validacion cruzada, pero puede ser util para comparar
                                                                o visualizar datos de modelos"

                                                                "Se le puede aplicar a cv=KFold() para que sea igual a
                                                                cross_val_score"


from sklearn.model_selection import Ridge

Ridge()  Ridge(columna, max_iter=100)                          "Es un otro tipo de regresion que tiene penalizacion por
                                                                por un parametro beta y otro lamnda, lo que voy a buscar
                                                                en esta regresion es que los parametros sean pequeños, se
                                                                puede minimizar"

from sklearn.model_selection import Lasso

Lasso()  Lasso(columna, max_iter=100)                          "Es otra regresion, pero esta no se puede minimizar con
                                                                derivada, hay que probar por intento, el beneficio es
                                                                que se puede anular un parametro en la minimizacion"

from sklearn.model_selection import GridSearchCV

GridSearch()  GridSearch(estimador, hiperparametros, cv=5)     "Encontrara los mejores parametros para ese estimador y para
                                                                esos hiperparametros, generalmente se ocupa para forest y
                                                                esa estimacion, pueden ser diccionarios y tener varios modelos,
                                                                etc"

                                                                Muy util para encontrar en una regresion polinomica que
                                                                grado queda mejor para una regresion o en caso de regresiones
                                                                ridge o lasso los mejores parametros beta para esos modelos,
                                                                porque se pueden probar varios a la vez

                                                                grid = GridSearch(...)

                                                                pd.DataFrame(grid.cv_results_) #muestra como dataframe donde
                                                                se le añaden nuevas columnas con puntuaciones

                                                                grid.fit(x,y)

                                                                grid.best_score_ , grid.best_params_   #Muestras los mejores
                                                                                                        hiperparametros 


---------------------------------------------------------------------------------------------------------------------------

                                                      PIPELINE

from sklearn.pipeline import Pipeline

Pipeline()  pipe = Pipeline([("nombre_operacion1", operacion1), ("nombre_operacion2", operacion2)])

            pipe.fit(x_train,y_train)   pipe.funcion() #añade otra funcion adicional

                                                               "Lo que hace es que puede hacer varias operaciones o funciones
                                                                a la vez, entendiendo por orden que primero se hace la primera
                                                                segundo la segunda, generalmente se ocupa un escalador y luego
                                                                un modelo de ajuste a eso se le llama estimador"

                                                                cross_val_predict(pipe, x, y, cv=KFold(....))

                                                                Para esto es bueno ocupar una pipeline

